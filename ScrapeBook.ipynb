{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Scrape/load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "import create_record\n",
    "import pandas as pd\n",
    "\n",
    "t1s114a1 = create_record.collect_data(1,108,1)\n",
    "t1s114a1[\"Text\"] = \" \".join([i for i in [i.get_text() for i in t1s114a1[\"SoupContent\"]]])\n",
    "\n",
    "madhahib_df = pd.read_csv(\"./data/madhahib.csv\", sep=\"\\t\", index_col=0)\n",
    "tafasir_df = pd.read_csv(\"./data/tafasir.csv\", sep=\"\\t\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# offline workaround\n",
    "import pandas as pd\n",
    "t1s114a2 = pd.read_csv('C:/Users/anaconda/Desktop/altafsir_scraper/corpus/1_1_7.csv', sep=\",\", index_col=0).T\n",
    "t1s114a2\n",
    "\n",
    "t1s114a1 = {}\n",
    "t1s114a1[\"Text\"] = t1s114a2[\"Text\"][0]\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "# 2 Preprocessing\n",
    "## 2.1 Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.utils.normalize import normalize_unicode\n",
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "\n",
    "# def remove_citations(text):\n",
    "#     import re\n",
    "#     text_without_citations = re.sub(\"([\\[\\{]).*?([\\]\\}])\", \"\", text)\n",
    "    \n",
    "#     return text_without_citations\n",
    "\n",
    "def reduce_to_archarset(text):\n",
    "    import re\n",
    "    # Remove non-arabic characters\n",
    "    nonarab_chars = '[^\\u0621-\\u064A ]'\n",
    "    text = re.sub(nonarab_chars, '', text)\n",
    "    return text\n",
    "\n",
    "## normalize\n",
    "def normalizer(string):\n",
    "    string = normalize_unicode(string)\n",
    "    string_normalized = normalize_alef_ar(string)\n",
    "    string_normalized = normalize_alef_maksura_ar(string_normalized)\n",
    "    string_normalized = normalize_teh_marbuta_ar(string_normalized)\n",
    "    \n",
    "    # remove diacritica\n",
    "    string_normalized = dediac_ar(string_normalized)\n",
    "    \n",
    "    #reduce to arabic charset\n",
    "    string_normalized = reduce_to_archarset(string_normalized)\n",
    "    return string_normalized\n",
    "\n",
    "t1s114a1[\"TextNormalized\"] = normalizer(t1s114a1[\"Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Simple Tokenization\n",
    "Will tokenize words by splitting the string on whitespace and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "\n",
    "t1s114a1[\"Tokenized\"] = simple_word_tokenize(t1s114a1[\"TextNormalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Morphological tokenization\n",
    "The morphological tokenizer expects pre-tokenized text in a list. Therefore run simple_word_tokenize(string) first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "\n",
    "\n",
    "# Load a pretrained disambiguator to use with a tokenizer\n",
    "mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "\n",
    "# `split=True`: morphological tokens are output as seperate strings.\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True)\n",
    "t1s114a1[\"TokenizedMorph\"] = tokenizer.tokenize(t1s114a1[\"Tokenized\"])\n",
    "\n",
    "# Rausfiltern unselbständiger Morpheme\n",
    "t1s114a1[\"TokenizedMorph\"] = [token for token in t1s114a1[\"TokenizedMorph\"] if not '+' in token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1s114a1[\"TokenizedMorph\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.3 Remove stopwords from token list]\n",
    "normalization needs to implemented for stoplist, before usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(token_list, stopwords_list, morph=True):\n",
    "\n",
    "    token_list_filtered = []\n",
    "    words_removed = []\n",
    "    with open (stopwords_list, encoding=\"UTF-8\") as stopwords:\n",
    "        if morph == True:\n",
    "            x = stopwords.read()\n",
    "        else:\n",
    "            x = normalizer(stopwords.read())\n",
    "        for token in token_list:\n",
    "            if token not in x:\n",
    "                token_list_filtered.append(token)\n",
    "            if token in x:\n",
    "                words_removed.append(token)\n",
    "\n",
    "    return token_list_filtered, words_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1s114a1[\"TokenizedMorphStopword\"] = stopwords(t1s114a1[\"TokenizedMorph\"], 'C:/Users/anaconda/Desktop/arabic-stop-words-master/list2.txt')[0]\n",
    "t1s114a1[\"TokenizedStopword\"] = stopwords(t1s114a1[\"Tokenized\"], 'C:/Users/anaconda/Desktop/arabic-stop-words-master/list2.txt', morph=False)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 3 Analyzing data\n",
    "## 3.1 Morphological analysis of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from camel_tools.morphology.database import MorphologyDB\n",
    "from camel_tools.morphology.analyzer import Analyzer\n",
    "\n",
    "\n",
    "# set up morphological analyzer\n",
    "db = MorphologyDB.builtin_db(\"calima-msa-r13\")\n",
    "analyzer = Analyzer(db)\n",
    "\n",
    "# create dictionary for every token taking index value in t1s114a1[\"tokenized\"] as key\n",
    "t1s114a1_token_analysis = {}\n",
    "for i, val in enumerate(t1s114a1[\"Tokenized\"]):\n",
    "    t1s114a1_token_analysis[i] = pd.DataFrame(analyzer.analyze(t1s114a1[\"Tokenized\"][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1s114a1_token_analysis[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create with root as key and frequency count as val\n",
    "def frequency_analyzer(token_list):\n",
    "    token_freqs = {}\n",
    "    for token in token_list:\n",
    "        if token in token_freqs:\n",
    "            token_freqs[token] += 1\n",
    "        else:\n",
    "            token_freqs[token] = 1\n",
    "\n",
    "    # sorting\n",
    "    token_freqs = {k: v for k, v in sorted(token_freqs.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 ... of morphologically tokenized tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_analyzer(t1s114a1[\"TokenizedMorphStopword\"])\n",
    "# frequency_analyzer(t1s114a1[\"TokenizedStopword\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 ... of roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of roots for the lemma with max probability for a given token from tokens_analysis\n",
    "t1s114a1[\"RootsList\"] = []\n",
    "for i in range(1, len(t1s114a1[\"Tokenized\"])):\n",
    "    if \"lex_logprob\" in t1s114a1_token_analysis[i]:\n",
    "        t1s114a1[\"RootsList\"].append(t1s114a1_token_analysis[i][t1s114a1_token_analysis[i].lex_logprob == t1s114a1_token_analysis[i].lex_logprob.max()].iloc[0][\"root\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_analyzer(t1s114a1[\"RootsList\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Named-entity recognition\n",
    "Hint: it seems that simple tokenization is more apt for NER, as it doesn't remove enclitics like \"ك\" from parts of the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.ner import NERecognizer\n",
    "\n",
    "ner = NERecognizer.pretrained()\n",
    "\n",
    "# NERecognizer expects pre-tokenized text\n",
    "sentence = t1s114a1[\"Tokenized\"] # simple_word_tokenize(t1s114a1[\"Text\"])\n",
    "\n",
    "labels = ner.predict_sentence(sentence)\n",
    "\n",
    "# save each token paired with it's NER label\n",
    "zipped = list(zip(sentence, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and glue named entities into dictionary\n",
    "named_entities = {\"LOC\" : [], \"ORG\" : [], \"PERS\" : [], \"MISC\" : []}\n",
    "for i, val in enumerate(zipped):\n",
    "    if zipped[i][1][0] == \"B\":\n",
    "        named_entities[zipped[i][1][2:]].append(zipped[i][0])\n",
    "    if zipped[i][1][0] == \"I\":\n",
    "        named_entities[zipped[i][1][2:]][-1] = named_entities[zipped[i][1][2:]][-1] + \" \" + zipped[i][0]\n",
    "named_entities[\"MISC\"]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc97be64c9ee71bc581e1e9ac6a692ed27a59201610f596c77b9adfd31c21240"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
